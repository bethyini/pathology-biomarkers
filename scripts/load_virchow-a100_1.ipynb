{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc069fc1-8705-4a70-a23b-722c0c75c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.layers import SwiGLUPacked\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa982ab-49c7-4131-a5d2-27f42aa8483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n"
     ]
    }
   ],
   "source": [
    "# for multi-gpu\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a4b40c-16a1-4064-a696-70aa9bdeb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\n",
    "    \"hf-hub:paige-ai/Virchow2\",\n",
    "    pretrained=True,\n",
    "    mlp_layer=SwiGLUPacked,\n",
    "    act_layer=torch.nn.SiLU\n",
    ")\n",
    "model.eval().cuda()\n",
    "\n",
    "config = resolve_data_config(model.pretrained_cfg, model=model)\n",
    "transform = create_transform(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95317b88-b12c-4d6d-b88a-c52d37ece4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TileFolderDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.paths = sorted([\n",
    "            os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".png\")\n",
    "        ])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        return self.transform(img), self.paths[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474f8e33-52c9-483f-95e4-8a8191363d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────── Embedding + Saving ─────────────\n",
    "def extract_and_save(tile_folder, h5_output_path, batch_size=96):\n",
    "    dataset = TileFolderDataset(tile_folder)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=20, pin_memory=True, prefetch_factor=2, persistent_workers=True)\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_coords = []\n",
    "\n",
    "    for batch_imgs, batch_paths in tqdm(dataloader, desc=os.path.basename(tile_folder)):\n",
    "        batch_imgs = batch_imgs.cuda()\n",
    "        with torch.no_grad():\n",
    "            out = model(batch_imgs)  # shape: (B, 261, 1280)\n",
    "\n",
    "        cls = out[:, 0]\n",
    "        patch_tokens = out[:, 5:]  # skip register tokens\n",
    "        mean = patch_tokens.mean(dim=1)\n",
    "        embedding = torch.cat([cls, mean], dim=-1)  # (B, 2560)\n",
    "        all_embeddings.append(embedding.cpu())\n",
    "\n",
    "        # Extract x, y from filename (e.g., TCGA-XX_L1_1232_2048.png)\n",
    "        for path in batch_paths:\n",
    "            base = os.path.splitext(os.path.basename(path))[0]\n",
    "            try:\n",
    "                x, y = map(int, base.split(\"_\")[-2:])\n",
    "            except:\n",
    "                x, y = 0, 0\n",
    "            all_coords.append((x, y))\n",
    "\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)     # (N, 2560)\n",
    "    all_coords = torch.tensor(all_coords)                 # (N, 2)\n",
    "\n",
    "    with h5py.File(h5_output_path, \"w\") as f:\n",
    "        f.create_dataset(\"features\", data=all_embeddings.numpy())\n",
    "        f.create_dataset(\"coords\", data=all_coords.numpy())\n",
    "\n",
    "    print(f\"✅ Saved {all_embeddings.shape[0]} embeddings to {h5_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7987e4f7-a8f5-40eb-aff9-047e9ea6576a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/orcd/data/edboyden/002/ezh/uni'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366fbcb-f4fa-435e-ad92-13194b552e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to process .ipynb_checkpoints: torch.cat(): expected a non-empty list of Tensors\n",
      "✅ Skipping TCGA-CK-4952, already exists.\n",
      "✅ Skipping TCGA-CK-5912, already exists.\n",
      "✅ Skipping TCGA-CK-5913, already exists.\n",
      "✅ Skipping TCGA-CK-5914, already exists.\n",
      "✅ Skipping TCGA-CK-5915, already exists.\n",
      "✅ Skipping TCGA-CK-5916, already exists.\n",
      "✅ Skipping TCGA-CK-6746, already exists.\n",
      "✅ Skipping TCGA-CK-6747, already exists.\n",
      "✅ Skipping TCGA-CK-6748, already exists.\n",
      "✅ Skipping TCGA-CK-6751, already exists.\n",
      "✅ Skipping TCGA-CL-4957, already exists.\n",
      "✅ Skipping TCGA-CL-5917, already exists.\n",
      "✅ Skipping TCGA-CL-5918, already exists.\n",
      "✅ Skipping TCGA-CM-4743, already exists.\n",
      "✅ Skipping TCGA-CM-4744, already exists.\n",
      "✅ Skipping TCGA-CM-4746, already exists.\n",
      "✅ Skipping TCGA-CM-4747, already exists.\n",
      "✅ Skipping TCGA-CM-4748, already exists.\n",
      "✅ Skipping TCGA-CM-4750, already exists.\n",
      "✅ Skipping TCGA-CM-4751, already exists.\n",
      "✅ Skipping TCGA-CM-4752, already exists.\n",
      "✅ Skipping TCGA-CM-5341, already exists.\n",
      "✅ Skipping TCGA-CM-5344, already exists.\n",
      "✅ Skipping TCGA-CM-5348, already exists.\n",
      "✅ Skipping TCGA-CM-5349, already exists.\n",
      "✅ Skipping TCGA-CM-5860, already exists.\n",
      "✅ Skipping TCGA-CM-5861, already exists.\n",
      "✅ Skipping TCGA-CM-5862, already exists.\n",
      "✅ Skipping TCGA-CM-5863, already exists.\n",
      "✅ Skipping TCGA-CM-5864, already exists.\n",
      "✅ Skipping TCGA-CM-5868, already exists.\n",
      "✅ Skipping TCGA-CM-6161, already exists.\n",
      "✅ Skipping TCGA-CM-6162, already exists.\n",
      "✅ Skipping TCGA-CM-6163, already exists.\n",
      "✅ Skipping TCGA-CM-6164, already exists.\n",
      "✅ Skipping TCGA-CM-6165, already exists.\n",
      "✅ Skipping TCGA-CM-6166, already exists.\n",
      "✅ Skipping TCGA-CM-6167, already exists.\n",
      "✅ Skipping TCGA-CM-6168, already exists.\n",
      "✅ Skipping TCGA-CM-6169, already exists.\n",
      "✅ Skipping TCGA-CM-6170, already exists.\n",
      "✅ Skipping TCGA-CM-6171, already exists.\n",
      "✅ Skipping TCGA-CM-6172, already exists.\n",
      "✅ Skipping TCGA-CM-6674, already exists.\n",
      "✅ Skipping TCGA-CM-6675, already exists.\n",
      "✅ Skipping TCGA-CM-6676, already exists.\n",
      "✅ Skipping TCGA-CM-6677, already exists.\n",
      "✅ Skipping TCGA-CM-6678, already exists.\n",
      "✅ Skipping TCGA-CM-6679, already exists.\n",
      "✅ Skipping TCGA-CM-6680, already exists.\n",
      "✅ Skipping TCGA-D5-5537, already exists.\n",
      "✅ Skipping TCGA-D5-5538, already exists.\n",
      "✅ Skipping TCGA-D5-5539, already exists.\n",
      "✅ Skipping TCGA-D5-5540, already exists.\n",
      "✅ Skipping TCGA-D5-5541, already exists.\n",
      "✅ Skipping TCGA-D5-6529, already exists.\n",
      "✅ Skipping TCGA-D5-6530, already exists.\n",
      "✅ Skipping TCGA-D5-6531, already exists.\n",
      "✅ Skipping TCGA-D5-6532, already exists.\n",
      "✅ Skipping TCGA-D5-6533, already exists.\n",
      "✅ Skipping TCGA-D5-6534, already exists.\n",
      "✅ Skipping TCGA-D5-6535, already exists.\n",
      "✅ Skipping TCGA-D5-6536, already exists.\n",
      "✅ Skipping TCGA-D5-6537, already exists.\n",
      "✅ Skipping TCGA-D5-6538, already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TCGA-D5-6539: 100%|█████████████████████████████████████████████████████| 96/96 [06:27<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 18715 embeddings to virchow_features/TCGA-D5-6539.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TCGA-D5-6540: 100%|█████████████████████████████████████████████████████| 96/96 [06:25<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 18715 embeddings to virchow_features/TCGA-D5-6540.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TCGA-D5-6541: 100%|█████████████████████████████████████████████████████| 96/96 [06:25<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 18715 embeddings to virchow_features/TCGA-D5-6541.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TCGA-D5-6898: 100%|█████████████████████████████████████████████████████| 25/25 [01:41<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 4832 embeddings to virchow_features/TCGA-D5-6898.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TCGA-D5-6920: 100%|█████████████████████████████████████████████████████| 19/19 [01:16<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 3630 embeddings to virchow_features/TCGA-D5-6920.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TCGA-D5-6922: 100%|█████████████████████████████████████████████████████| 25/25 [01:39<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 4710 embeddings to virchow_features/TCGA-D5-6922.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TCGA-D5-6923: 100%|█████████████████████████████████████████████████████| 21/21 [01:25<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 4050 embeddings to virchow_features/TCGA-D5-6923.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TCGA-D5-6924: 100%|█████████████████████████████████████████████████████| 28/28 [01:54<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 5460 embeddings to virchow_features/TCGA-D5-6924.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TCGA-D5-6926: 100%|█████████████████████████████████████████████████████| 27/27 [01:48<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 5184 embeddings to virchow_features/TCGA-D5-6926.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TCGA-D5-6927: 100%|█████████████████████████████████████████████████████| 25/25 [01:40<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 4788 embeddings to virchow_features/TCGA-D5-6927.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TCGA-D5-6928: 100%|███████████████████████████████████████████████████████| 9/9 [00:36<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1680 embeddings to virchow_features/TCGA-D5-6928.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TCGA-D5-6929:  22%|███████████▊                                         | 17/76 [01:11<03:56,  4.01s/it]"
     ]
    }
   ],
   "source": [
    "tile_root_dir = \"virchow_tiles_gpu0\"               # root directory containing subfolders for each WSI\n",
    "output_dir = \"virchow_features\"                   # where to save .h5 files\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for slide_folder in sorted(os.listdir(tile_root_dir)):\n",
    "    slide_path = os.path.join(tile_root_dir, slide_folder)\n",
    "    if not os.path.isdir(slide_path):\n",
    "        continue  # skip files\n",
    "\n",
    "    h5_output_path = os.path.join(output_dir, f\"{slide_folder}.h5\")\n",
    "\n",
    "    if os.path.exists(h5_output_path):\n",
    "        print(f\"✅ Skipping {slide_folder}, already exists.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        extract_and_save(slide_path, h5_output_path, batch_size=196)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to process {slide_folder}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLAM",
   "language": "python",
   "name": "clam_latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
